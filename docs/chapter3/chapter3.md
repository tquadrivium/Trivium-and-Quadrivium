# 第三章：复杂性

*Edit: 王茂霖，李一飞，Hao ZHAN*

*Update: 06/12/2020*

---

## 1. 【概念补充】shattering概念的可视化

**P40**中提到了shattering的概念。即，若假设空间能够实现样本集上所有对分，则称样本集能够被该假设空间**打散（shattering）**。

这里用 $R^2$ 空间的示例进一步介绍打散的概念，以二维空间 $R^2$ 来说，一条任意的直线 $w_1x_1+w_2x_2+b=0$ 对于任意的三个点实现的对分（二分类）的所有情形如下：

<center><img src="http://haozhan93.gitee.io/image-host/mlt/docs/chapter3/img/shattering.png" width= "600"/></center>



可以发现，使用二维平面 $R^2$ 中的一条直线 $sign(wx+b)$ 可以实现三点的所有对分。

同样，考察四个点的情况，发现直线 $sign(wx+b)$ 并不能够实现对任意四点的对分。

例如，异或（XOR）问题：

<center><img src="http://haozhan93.gitee.io/image-host/mlt/docs/chapter3/img/xor.png" width= "300"/></center>



因此，根据VC维的定义， $sign(wx+b)$ 这样一个 $R^2$ 中的非齐次超平面的VC维为3。而对于齐次超平面 $sign(wx)$ ，由于其必须过原点，所以只能满足对任意两点的对分，因此其VC维为2。

## 2.【证明补充】定理3.5和定理3.6补充

**P50-P51**的**定理3.5**和**定理3.6**主要对线性超平面的VC维进行分析。

假设空间的VC维是能被 $h$ 打散的最大样本集的大小，如果其VC维为 $d$, 那么意味着：

- 存在一个大小d的样本集能被 $h$ 打散。因此在进行VC维的证明工作时，需要构造一个大小为d的样本集，证明在该样本集能够被打散。*Tips: 这也说明如果一个假设空间的VC维为d,其并不代表能打散所有大小为的d的样本集.*

- 对于任意的d+1维样本不能被其打散。一般用反证法进行证明。

基于此，就可以展开对 $R^d$ 中的齐次的线性超平面的考察。

**定理3.5**的证明，是通过构造 $R^d$ 中的 $d$ 个的单位向量组成了一个大小为d的样本集 $D=\{e_1,...,e_d\}$，然后利用超平面的可打散性证明了这个样本集的VC维至少是 $d$。然后，利用反证法说明对于任意 $R^d$ 中的 $d+1$ 个样本集是不能打散的。最终证明 $R^d$ 中的齐次的线性超平面的VC维为 $d$。

对于非齐次的利用升维转化为齐次可以利用其次的结论同证。

## 3.【证明补充】定理3.8补充

**P52**定理3.8给出一个和空间维数无关的VC维刻画，这里对其不等式的思想和意义进行介绍。

**定理3.8**对于超平面族的刻画，使其突破了与空间维数的关系。也就是说超平面的VC维不会随着空间维数的变大而增加。这里似乎与**定理3.6**的结论矛盾：$R^d$ 中的 $sign(wx+d)$ 维数至少为 $d+1$ 维，而通过高斯核映射后，维数 $d$ 扩至无穷维。按照定理3.6的结论，那假设空间的维数也是无穷维。

但是**定理3.8**会认为说，由于SVM的核函数空间映射转化，**定理3.6**的结论将会没有意义——无穷维的VC维度当然是无意义的。因此，**定理3.8**利用了对于 $w$ 的约束，从而对于超平面进行了限制。在限制了超平面后，其VC维数就会降低(模型可以表达的复杂度降低)。而降低后的维数是不能够直接求解得出的，而只能够得到其VC维的上界。

此外，使用**Novikoff定理**证明感知机的收敛性，可得到相同的不等式。不同的是，**Novikoff定理**证明感知机的收敛性时，不等式的左侧为感知机算法在数据集上的误分类次数。

## 4.【证明补充】多层神经网络的复杂性的描述思路

描述多层神经网络的复杂度，需要通过两个步骤来完成。

首先，需要描述单个映射函数的增长函数，即**式3.51**的内容；其次，需要考察多层映射下的增长函数，即**引理3.2**，**引理3.3**，及**定理3.9**的内容。

其中，**引理3.2** 构造了函数族的增长函数，与其笛卡尔积的增长函数的关系；**引理3.3** 在**引理3.2**的基础之上，扩展到复合函数族的情况；最终**定理3.9**再一般化到多层神经网络的情况。



## 部分参考文献

[1] Understanding Machine Learning：3

[2] Foundations of Machine Learning：3，4

[3] Statistical Learning Theory：9.2